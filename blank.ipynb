{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## template notebook\n",
    "\n",
    "Fill me in with details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'gut'\n",
    "acc = 'p8808mo11'\n",
    "ksize=31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metag_filename = f'{name}/{acc}.abundtrim.sig'\n",
    "queries_filename = f'{name}/{name}-queries.zip'\n",
    "nbhds_filename = f'{name}/{name}-nbhds.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sourmash\n",
    "import csv\n",
    "import subprocess\n",
    "import os\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ident(name):\n",
    "    \"pick off identifier, stripping off nbhd: prefix if present.\"\n",
    "    name = name.split(' ')[0]\n",
    "    if name.startswith('nbhd:'):\n",
    "        name = name[5:]\n",
    "    return name\n",
    "\n",
    "def load_gather(filename):\n",
    "    gather_rows = []\n",
    "    with open(filename, newline=\"\") as fp:\n",
    "        r = csv.DictReader(fp)\n",
    "        gather_rows.extend(r)\n",
    "\n",
    "    full_idents = {}\n",
    "\n",
    "    gather_d = {}\n",
    "    gather_keys = []\n",
    "    for row in gather_rows:\n",
    "        ident = get_ident(row['name'])\n",
    "        full_idents[ident] = \" \".join(row['name'].split(' ')[:3])\n",
    "        gather_d[ident] = row\n",
    "        gather_keys.append(ident)\n",
    "        \n",
    "    return gather_keys, gather_d, full_idents\n",
    "\n",
    "def gather_and_load(name, metag_filename, query_sigs, nbhd_sigs, ksize=31):\n",
    "    assert os.path.exists(metag_filename)\n",
    "    assert os.path.exists(query_sigs)\n",
    "    assert os.path.exists(nbhd_sigs)\n",
    "    \n",
    "    query_out = f\"{name}/queries.gather.csv\"\n",
    "    nbhd_out = f\"{name}/nbhd.gather.csv\"\n",
    "    stdout1_out = f\"{name}/queries.gather.out\"\n",
    "    stdout2_out = f\"{name}/nbhd.gather.out\"\n",
    "    \n",
    "    cmd = f\"sourmash gather -k {ksize} {metag_filename} {query_sigs} -o {query_out} --ignore-abundance >& {stdout1_out}\"\n",
    "    print(cmd)\n",
    "    if not os.path.exists(query_out):\n",
    "        result = subprocess.run(cmd, shell=True)\n",
    "    else:\n",
    "        print(f\"** {query_out} already exists; not rerunning sourmash gather on queries\")\n",
    "        \n",
    "    stdout1 = open(stdout1_out, 'rt').read()\n",
    "\n",
    "\n",
    "    cmd = f\"sourmash gather -k {ksize} {metag_filename} {nbhd_sigs} -o {nbhd_out} --ignore-abundance >& {stdout2_out}\"\n",
    "    print(cmd)\n",
    "    if not os.path.exists(nbhd_out):\n",
    "        result = subprocess.run(cmd, shell=True)\n",
    "    else:\n",
    "        print(f\"** {nbhd_out} already exists; not rerunning sourmash gather on nbhds\")\n",
    "        \n",
    "    stdout2 = open(stdout2_out, 'rt').read()\n",
    "\n",
    "       \n",
    "    gather1_keys, gather1_d, full_idents = load_gather(query_out)\n",
    "    gather2_keys, gather2_d, _ = load_gather(nbhd_out)\n",
    "    \n",
    "    metag = sourmash.load_one_signature(metag_filename, ksize=ksize)\n",
    "    \n",
    "    queries = list(sourmash.load_file_as_signatures(query_sigs))\n",
    "    queries_d = {}\n",
    "    for ss in queries:\n",
    "        ident = get_ident(ss.name)\n",
    "        queries_d[ident] = ss\n",
    "        \n",
    "    nbhds = list(sourmash.load_file_as_signatures(nbhd_sigs))\n",
    "    nbhds_d = {}\n",
    "    for ss in nbhds:\n",
    "        ident = get_ident(ss.name)\n",
    "        nbhds_d[ident] = ss\n",
    "        \n",
    "    tup = collections.namedtuple('augmented', 'metag, queries_d, nbhds_d, gather1_keys, gather1_d, gather2_keys, gather2_d, full_idents, stdout1, stdout2')\n",
    "    t = tup(metag, queries_d, nbhds_d, gather1_keys, gather1_d, gather2_keys, gather2_d, full_idents, stdout1, stdout2)\n",
    "    \n",
    "    return t\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = gather_and_load(name, metag_filename, queries_filename, nbhds_filename, ksize=ksize)\n",
    "metag, queries_d, nbhds_d, gather1_keys, gather1_d, gather2_keys, gather2_d, full_idents, stdout1, stdout2 = t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gather output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stdout1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stdout2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building mapping from query to neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_hashes = set(metag.minhash.hashes)\n",
    "unique_hashes_1x = []\n",
    "for ident in gather1_keys:\n",
    "    row = gather1_d[ident]\n",
    "    match = queries_d[ident]\n",
    "    match_hashes = set(match.minhash.hashes)\n",
    "    unique_overlap = remaining_hashes & match_hashes\n",
    "    #print(ident, len(remaining_hashes), len(unique_overlap), row['unique_intersect_bp'], row['remaining_bp'])\n",
    "    remaining_hashes -= unique_overlap\n",
    "    unique_hashes_1x.append((ident, unique_overlap))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_hashes = set(metag.minhash.hashes)\n",
    "unique_hashes_2x = []\n",
    "for ident in gather2_keys:\n",
    "    row = gather2_d[ident]\n",
    "    match = nbhds_d[ident]\n",
    "    match_hashes = set(match.minhash.hashes)\n",
    "    unique_overlap = remaining_hashes & match_hashes\n",
    "    #print(ident, len(remaining_hashes), len(unique_overlap), row['unique_intersect_bp'], row['remaining_bp'])\n",
    "    remaining_hashes -= unique_overlap\n",
    "    unique_hashes_2x.append((ident, unique_overlap))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (nbhd_ident, nbhd_match) in unique_hashes_2x:\n",
    "    total = 0\n",
    "    for (query_ident, query_match) in unique_hashes_1x:\n",
    "        overlap = query_match & nbhd_match\n",
    "        \n",
    "        if overlap:\n",
    "            print(f\"{nbhd_ident} <= {query_ident} - {len(overlap)}\")\n",
    "            total += len(overlap)\n",
    "    #print('xxx', nbhd_ident, len(nbhd_match), total)\n",
    "    #print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def make_fig(max_num=None):\n",
    "    #labels = obj.make_labels()\n",
    "    #src_l, dest_l, cnt_l, color_l, label_l = obj.make_lists()\n",
    "    labels = []\n",
    "    src_l = []\n",
    "    dest_l = []\n",
    "    cnt_l = []\n",
    "    color_l = []\n",
    "    label_l = []\n",
    "    \n",
    "    source_idx = {}\n",
    "    for n, (query_ident, _) in enumerate(unique_hashes_1x):\n",
    "        source_idx[query_ident] = n\n",
    "        labels.append(full_idents[query_ident])\n",
    "    dest_idx = {}\n",
    "    source_idx[\"unassigned\"] = len(unique_hashes_1x)\n",
    "    labels.append(\"unassigned\")\n",
    "    base = len(unique_hashes_1x) + 1\n",
    "    for n, (nbhd_ident, _) in enumerate(unique_hashes_2x):\n",
    "        dest_idx[nbhd_ident] = base + n\n",
    "        labels.append(full_idents[nbhd_ident])\n",
    "        \n",
    "    #source_idx[\"unassigned\"] = base + n + 1\n",
    "    \n",
    "    # iterate over all sinks, account for all sources\n",
    "    num = 0\n",
    "    leftovers = []\n",
    "    for (nbhd_ident, nbhd_match) in unique_hashes_2x:\n",
    "        total = 0\n",
    "        for (query_ident, query_match) in unique_hashes_1x:\n",
    "            overlap = query_match & nbhd_match\n",
    "        \n",
    "            if overlap:\n",
    "                #print(f\"{nbhd_ident} <= {query_ident} - {len(overlap)}\")\n",
    "                total += len(overlap)\n",
    "                from_idx = source_idx[query_ident]\n",
    "                to_idx = dest_idx[nbhd_ident]\n",
    "                \n",
    "                src_l.append(from_idx)\n",
    "                dest_l.append(to_idx)\n",
    "                cnt_l.append(len(overlap))\n",
    "                if query_ident == nbhd_ident:\n",
    "                    color_l.append(\"lightseagreen\")\n",
    "                else:\n",
    "                    color_l.append(\"palevioletred\")\n",
    "                label_l.append(\"\")\n",
    "    \n",
    "        #print('xxx', nbhd_ident, len(nbhd_match), total)\n",
    "        leftover = len(nbhd_match) - total\n",
    "        \n",
    "        if leftover:\n",
    "            leftovers.append((nbhd_ident, leftover))\n",
    "\n",
    "        #print('---')\n",
    "        num += 1\n",
    "        if max_num and num >= max_num:\n",
    "            break\n",
    "            \n",
    "    if leftovers:\n",
    "        for nbhd_ident, leftover in leftovers:\n",
    "            from_idx = source_idx[\"unassigned\"]\n",
    "            to_idx = dest_idx[nbhd_ident]\n",
    "\n",
    "            src_l.append(from_idx)\n",
    "            dest_l.append(to_idx)\n",
    "            cnt_l.append(leftover)\n",
    "            color_l.append(\"grey\")\n",
    "            label_l.append(\"\")\n",
    "            #print('XYZ', from_idx, to_idx)\n",
    "        \n",
    "\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        node = dict(\n",
    "          pad = 15,\n",
    "          thickness = 20,\n",
    "          line = dict(color = \"black\", width = 0.5),\n",
    "          label = labels,\n",
    "          color = \"blue\"\n",
    "        ),\n",
    "        link = dict(\n",
    "          source = src_l,\n",
    "          target = dest_l,\n",
    "          value = cnt_l,\n",
    "          color = color_l,\n",
    "          label = label_l,\n",
    "      ))])\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM=5\n",
    "fig = make_fig(NUM)\n",
    "\n",
    "fig.update_layout(title_text=f\"original genome contributions to top {NUM} neighborhood covers for {name} ({acc})\", font_size=10)\n",
    "fig.update_layout(width=800, height=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
